{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "artificial_neural_network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPlogEH/ggrDYPE/RfejXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurtispykes/ml-from-scratch/blob/master/artificial_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3eRQSX0yyh9p"
      },
      "outputs": [],
      "source": [
        "import typing as t\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# creating a train and test dataset \n",
        "X_train, y_train = make_circles(n_samples=10000, noise=.05)\n",
        "X_test, y_test = make_circles(n_samples=200, noise=.05)\n",
        "\n",
        "# transposing for simplicity\n",
        "X_train, X_test = X_train.T, X_test.T\n",
        "y_train, y_test = y_train.reshape((1, y_train.shape[0])), y_test.reshape((1, y_test.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def he_initialization(layer_dims: list, \n",
        "                      random_state: t.Optional[int]=None\n",
        "                      ) -> dict: \n",
        "  \"\"\"\n",
        "  Initialize neural network weights using he initialization\n",
        "  (http://arxiv.org/abs/1502.01852) from a uniform distribution.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  layer_dims: list \n",
        "    A list detailing the number of neurons in each layer. The length of the\n",
        "    list determines the number of layers in the network (including input layer).\n",
        "  random_state: int\n",
        "    A seed to generate the same random numbers.\n",
        "\n",
        "  Return\n",
        "  ----------\n",
        "  parameters: dict\n",
        "    A dictionary containing the initialized weights. \n",
        "    Disclaimer - bias weights are all initalized to zero. \n",
        "  \"\"\"\n",
        "  np.random.seed(random_state)\n",
        "  parameters = {}\n",
        "  num_layers = len(layer_dims)\n",
        "\n",
        "  for i in range(1, num_layers): \n",
        "    limit = np.sqrt(2/float(layer_dims[i-1]))\n",
        "    parameters[f\"W{i}\"] = np.random.uniform(low=-limit, high=limit, size=(layer_dims[i], layer_dims[i-1]))\n",
        "    parameters[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
        "  return parameters\n",
        "  \n",
        "# initalizing 3 layer NN weights\n",
        "params = he_initialization([X_train.shape[0], 4, 3, 1])\n",
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xYYkF-83g3W",
        "outputId": "d4bd8bed-91f9-4185-9024-a71549490cd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.38983035, -0.89453521],\n",
              "        [-0.58021488, -0.25304912],\n",
              "        [ 0.4383582 ,  0.33055966],\n",
              "        [-0.16004868, -0.77240534]]),\n",
              " 'W2': array([[ 0.44811701,  0.33828157, -0.39110288, -0.00487558],\n",
              "        [ 0.58353667, -0.34552995, -0.15289273,  0.03661937],\n",
              "        [-0.02783659, -0.40419987,  0.54664764,  0.00860773]]),\n",
              " 'W3': array([[ 0.57832271, -0.25381305, -0.69840973]]),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b3': array([[0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Weight 1: {params['W1'].shape}\\nWeight 2: {params['W2'].shape}\\nWeight 3: {params['W3'].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgHgdEnI2QlU",
        "outputId": "40ccbe30-20c9-46f4-ac93-7e2e9651a8f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight 1: (4, 2)\n",
            "Weight 2: (3, 4)\n",
            "Weight 3: (1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z: t.Optional[t.Union[int, np.array]]\n",
        "         ) -> t.Optional[t.Union[int, np.array]]:\n",
        "  \"\"\"\n",
        "  ReLU activation function.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  Z: t.Optional[t.Union[int, np.array]\n",
        "    A numpy array or scalar value of any size\n",
        "\n",
        "  Return\n",
        "  ----------\n",
        "  parameters: dict\n",
        "    A dictionary containing the initialized weights. \n",
        "    Disclaimer - bias weights are all initalized to zero.\n",
        "    \n",
        "  init_params: dict\n",
        "    A dictionary containing the initialized weights. \n",
        "  \"\"\" \n",
        "  return np.maximum(0, Z)\n",
        "\n",
        "def sigmoid(Z: t.Optional[t.Union[int, np.array]]\n",
        "             ) -> t.Optional[t.Union[int, np.array]]:     \n",
        "  \"\"\"\n",
        "  Sigmoid activation function.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  Z: t.Optional[t.Union[int, np.array]\n",
        "    A numpy array or scalar value of any size\n",
        "\n",
        "  Return\n",
        "  ----------\n",
        "  parameters: dict\n",
        "    A dictionary containing the initialized weights. \n",
        "    Disclaimer - bias weights are all initalized to zero.\n",
        "    \n",
        "  init_params: dict\n",
        "    A dictionary containing the initialized weights.  \n",
        "  \"\"\"\n",
        "  Z = Z.astype(np.float128)\n",
        "  return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def forward_pass(x:np.array,\n",
        "                 parameters:dict\n",
        "                 ) -> dict:\n",
        "  \"\"\"\n",
        "  Performs forward propagation\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: np.array\n",
        "    A numpy array of the given data. The shape of the data should be in\n",
        "    (n_features, n_samples)\n",
        "\n",
        "  Return\n",
        "  ----------\n",
        "  parameters: dict\n",
        "    A dictionary containing the initialized weights. \n",
        "    Disclaimer - bias weights are all initalized to zero.\n",
        "    \n",
        "  init_params: dict\n",
        "    A dictionary containing the initialized weights.  \n",
        "  \"\"\"\n",
        "  init_params = parameters.copy()\n",
        "\n",
        "  z1 = np.dot(parameters[\"W1\"], x) + parameters[\"b1\"]\n",
        "  a1 = relu(z1)\n",
        "  parameters[\"z1\"] = z1\n",
        "  parameters[\"a1\"] = a1\n",
        "\n",
        "  z2 = np.dot(parameters[\"W2\"], a1) + parameters[\"b2\"]\n",
        "  a2 = relu(z2)\n",
        "  parameters[\"z2\"] = z2\n",
        "  parameters[\"a2\"] = a2\n",
        "\n",
        "  z3 = np.dot(parameters[\"W3\"], a2) + parameters[\"b3\"]\n",
        "  z3 = z3.astype(dtype=\"float128\")\n",
        "  a3 = sigmoid(z3)\n",
        "  parameters[\"z3\"] = z3\n",
        "  parameters[\"a3\"] = a3\n",
        "\n",
        "  return parameters, init_params\n",
        "\n",
        "params = he_initialization([X_train.shape[0], 4, 3, 1])\n",
        "\n",
        "pred_params, init_params = forward_pass(X_train, params)\n",
        "pred_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4ncZ_Xa_sM_",
        "outputId": "9ee3a8ad-7e08-4056-ea2b-983dd7497c69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.43993575, -0.39819365],\n",
              "        [ 0.99157619, -0.3413676 ],\n",
              "        [ 0.33603766,  0.66091697],\n",
              "        [ 0.73666798, -0.22894142]]),\n",
              " 'W2': array([[ 0.56107414, -0.027116  , -0.3353572 , -0.47143681],\n",
              "        [ 0.61370231,  0.43138415, -0.46847495,  0.04270518],\n",
              "        [-0.22859831, -0.21950783,  0.53704251, -0.57185608]]),\n",
              " 'W3': array([[-0.64108134, -0.22988668, -0.15552054]]),\n",
              " 'a1': array([[0.11594041, 0.07395744, 0.56868031, ..., 0.41474893, 0.        ,\n",
              "         0.        ],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.503736  ,\n",
              "         0.        ],\n",
              "        [0.12768688, 0.11403679, 0.        , ..., 0.        , 0.69960757,\n",
              "         0.30775987],\n",
              "        [0.        , 0.        , 0.        , ..., 0.        , 0.39103953,\n",
              "         0.        ]]),\n",
              " 'a2': array([[0.02223045, 0.00325255, 0.31907182, ..., 0.2327049 , 0.        ,\n",
              "         0.        ],\n",
              "        [0.01133479, 0.        , 0.34900042, ..., 0.25453237, 0.        ,\n",
              "         0.        ],\n",
              "        [0.0420695 , 0.04433606, 0.        , ..., 0.        , 0.04152668,\n",
              "         0.16528013]]),\n",
              " 'a3': array([[0.49415029, 0.49775494, 0.42928191, ..., 0.44826176, 0.49838544,\n",
              "         0.49357424]], dtype=float128),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b3': array([[0.]]),\n",
              " 'z1': array([[ 0.11594041,  0.07395744,  0.56868031, ...,  0.41474893,\n",
              "         -0.59782515, -0.04040871],\n",
              "        [-1.01223393, -0.75885193, -0.03028064, ..., -0.0862348 ,\n",
              "          0.503736  , -0.87043807],\n",
              "        [ 0.12768688,  0.11403679, -0.79477282, ..., -0.56116843,\n",
              "          0.69960757,  0.30775987],\n",
              "        [-0.73706215, -0.55197932, -0.04741698, ..., -0.08096378,\n",
              "          0.39103953, -0.62752456]]),\n",
              " 'z2': array([[ 0.02223045,  0.00325255,  0.31907182, ...,  0.2327049 ,\n",
              "         -0.43262817, -0.10320949],\n",
              "        [ 0.01133479, -0.00803552,  0.34900042, ...,  0.25453237,\n",
              "         -0.09374548, -0.14417779],\n",
              "        [ 0.0420695 ,  0.04433606, -0.12999936, ..., -0.0948109 ,\n",
              "          0.04152668,  0.16528013]]),\n",
              " 'z3': array([[-0.02339992, -0.00898032, -0.28478153, ..., -0.20769637,\n",
              "         -0.00645825, -0.02570446]], dtype=float128)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_log_loss(y: np.array, y_hat: np.array) -> float:\n",
        "  \"\"\"\n",
        "  Compute the log loss\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  y: np.array\n",
        "    A numpy array of the true labels\n",
        "  y_hat: np.array\n",
        "    A numpy array of the predicted labels\n",
        "\n",
        "  Return\n",
        "  ----------\n",
        "  parameters: dict\n",
        "    A scalar value\n",
        "  \"\"\"\n",
        "  m = y.shape[1]\n",
        "  J = -1/m * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "  return J \n",
        "\n",
        "\n",
        "init_params = he_initialization([X_train.shape[0], 4, 3, 1])\n",
        "params, _ = forward_pass(X_train, init_params)\n",
        "\n",
        "compute_log_loss(y_train, params[\"a3\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80yowf6mkqHT",
        "outputId": "8afb3eb1-8625-4b80-f17f-2002d3d4020b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.68922612054351680083"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(x: np.array, y: np.array, parameters: dict) -> dict: \n",
        "    \"\"\"\n",
        "    Implement backpropagation \n",
        "    Credit: https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: np.array\n",
        "      A numpy array of the given data. The shape of the data should be in\n",
        "      (n_features, n_samples)\n",
        "    y: np.array\n",
        "      A numpy array of the true labels\n",
        "    parameters: dict\n",
        "      A dictionary containing weights from forward_pass()\n",
        "    \n",
        "    Return\n",
        "    ----------\n",
        "    gradients: dict\n",
        "      A dictionary containing the gradients with respect to each parameters,\n",
        "      activation, and pre-activation variables. \n",
        "    \"\"\"\n",
        "    m = x.shape[0]\n",
        "\n",
        "    dz3 = 1./m * (parameters[\"a3\"] - y)\n",
        "    dW3 = np.dot(dz3, parameters[\"a2\"].T)\n",
        "    db3 = np.sum(dz3, axis=0)\n",
        "    \n",
        "    da2 = np.dot(parameters[\"W3\"].T, dz3)\n",
        "    dz2 = np.multiply(da2, parameters[\"a2\"])\n",
        "    dW2 = np.dot(dz2, parameters[\"a1\"].T)\n",
        "    db2 = np.sum(dz2, axis=0)\n",
        "    \n",
        "    da1 = np.dot(parameters[\"W2\"].T, dz2)\n",
        "    dz1 = np.multiply(da1, parameters[\"a1\"])\n",
        "    dW1 = np.dot(dz1, x.T)\n",
        "    db1 = np.sum(dz1, axis=0)\n",
        "\n",
        "    gradients = {\n",
        "        \"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
        "        \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
        "        \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1\n",
        "    }\n",
        "    return gradients\n",
        "\n",
        "init_params = he_initialization([X_train.shape[0], 4, 3, 1])\n",
        "pred_params, init_params = forward_pass(X_train, init_params)\n",
        "backpropagation(X_train, y_train, pred_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wLmcAhzaZ-f",
        "outputId": "3b7037a0-48b6-40ea-c256-44fc4fde20e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dW1': array([[  2.51964   ,  -2.85549848],\n",
              "        [ -6.54161532,  -3.93686666],\n",
              "        [  4.91256695,  -0.8931207 ],\n",
              "        [-11.15760982,  -1.90368524]], dtype=float128),\n",
              " 'dW2': array([[-2.76071917, -6.70549891, -2.40279481, -5.98690275],\n",
              "        [-5.54025023, -2.28308989, -4.58328112, -0.56010956],\n",
              "        [-1.50618789, -0.06650729, -1.91162308, -0.30825201]],\n",
              "       dtype=float128),\n",
              " 'dW3': array([[-36.97683837, -12.44506037,  -4.67279508]], dtype=float128),\n",
              " 'da1': array([[-0.01775706,  0.01916655,  0.        , ..., -0.001486  ,\n",
              "          0.04137239, -0.02098434],\n",
              "        [ 0.00447583, -0.00466445,  0.        , ..., -0.00023058,\n",
              "         -0.02870756,  0.00395518],\n",
              "        [-0.01834904,  0.01871187,  0.        , ...,  0.00248553,\n",
              "         -0.05764787, -0.01292907],\n",
              "        [-0.00748378,  0.00733728,  0.        , ...,  0.00209699,\n",
              "         -0.05281241, -0.00291582]], dtype=float128),\n",
              " 'da2': array([[-0.11858993,  0.16488752, -0.14498877, ...,  0.14607891,\n",
              "         -0.11358538, -0.12210311],\n",
              "        [-0.10520529,  0.1462775 , -0.12862462, ...,  0.12959173,\n",
              "         -0.10076557, -0.10832195],\n",
              "        [-0.08061734,  0.1120904 , -0.09856324, ...,  0.09930432,\n",
              "         -0.07721524, -0.08300559]], dtype=float128),\n",
              " 'db1': array([-2.53313248e-02,  2.01150951e-02,  0.00000000e+00, ...,\n",
              "         5.85006226e-05, -6.54251751e-02, -2.25720966e-02], dtype=float128),\n",
              " 'db2': array([-0.07530147,  0.07853514,  0.        , ...,  0.00378773,\n",
              "        -0.08620524, -0.06702515], dtype=float128),\n",
              " 'db3': array([ 0.20448123, -0.28431084,  0.25      , ..., -0.2518797 ,\n",
              "         0.19585202,  0.21053891], dtype=float128),\n",
              " 'dz1': array([[-1.31323079e-02,  1.07690639e-02,  0.00000000e+00, ...,\n",
              "         -0.00000000e+00,  0.00000000e+00, -1.50324379e-02],\n",
              "        [ 0.00000000e+00, -0.00000000e+00,  0.00000000e+00, ...,\n",
              "         -0.00000000e+00, -2.91214294e-02,  0.00000000e+00],\n",
              "        [-1.21990169e-02,  9.34603119e-03,  0.00000000e+00, ...,\n",
              "          5.85006226e-05, -0.00000000e+00, -7.53965876e-03],\n",
              "        [-0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "          0.00000000e+00, -3.63037457e-02, -0.00000000e+00]],\n",
              "       dtype=float128),\n",
              " 'dz2': array([[-0.01063254,  0.01061111, -0.        , ...,  0.00229921,\n",
              "         -0.08620524, -0.00563731],\n",
              "        [-0.04585724,  0.04845417, -0.        , ...,  0.        ,\n",
              "         -0.        , -0.0458418 ],\n",
              "        [-0.0188117 ,  0.01946986, -0.        , ...,  0.00148852,\n",
              "         -0.        , -0.01554603]], dtype=float128),\n",
              " 'dz3': array([[ 0.20448123, -0.28431084,  0.25      , ..., -0.2518797 ,\n",
              "          0.19585202,  0.21053891]], dtype=float128)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(init_params: dict, \n",
        "                      gradients: dict,\n",
        "                      learning_rate: float\n",
        "                      ) -> dict:\n",
        "  \"\"\"\n",
        "  Perfrom parameter update \n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  init_param: dict\n",
        "    The original initialized parameters\n",
        "  gradients: dict\n",
        "    A dictionary containing the gradients with respect to each parameters,\n",
        "    activation, and pre-activation variables. \n",
        "  learning_rate: float\n",
        "    Step size of gradient descent\n",
        "  \n",
        "  Return\n",
        "  ----------\n",
        "  new_params: dict\n",
        "    A dictionary containing the updated parameters. \n",
        "  \"\"\"\n",
        "  num_layers = len(init_params) // 2\n",
        "  new_params = init_params.copy()\n",
        "\n",
        "  for i in range(1, num_layers-1): \n",
        "    new_params[f\"W{i}\"] = new_params[f\"W{i}\"] - learning_rate * gradients[f\"dW{i}\"]\n",
        "    new_params[f\"b{i}\"] = new_params[f\"b{i}\"] - learning_rate * gradients[f\"db{i}\"]\n",
        "  return new_params\n",
        "\n",
        "init_params = he_initialization([X_train.shape[0], 4, 3, 1])\n",
        "pred_params, init_params = forward_pass(X_train, init_params)\n",
        "update_params = backpropagation(X_train, y_train, pred_params)\n",
        "\n",
        "update_parameters(init_params, update_params, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8CAaUONdkxj",
        "outputId": "20070c94-aa50-4c2d-8346-07c00fd2b940"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[-0.1685307 , -0.18752917],\n",
              "        [-0.89300283, -0.69037631],\n",
              "        [ 0.24923499, -0.02502872],\n",
              "        [-0.46303863, -0.47070765]], dtype=float128),\n",
              " 'W2': array([[ 0.14267776,  0.14068145,  0.3735234 , -0.54227296],\n",
              "        [ 0.58697229,  0.0473108 ,  0.66061265, -0.06975443],\n",
              "        [-0.0893605 , -0.56183741,  0.50453268,  0.61193564]]),\n",
              " 'W3': array([[0.39246487, 0.37374352, 0.34222643]]),\n",
              " 'b1': array([[-4.54832083e-07,  1.59242107e-07, -2.50270314e-05, ...,\n",
              "          1.24920509e-05, -3.07494930e-05, -2.08607969e-09],\n",
              "        [-4.54832083e-07,  1.59242107e-07, -2.50270314e-05, ...,\n",
              "          1.24920509e-05, -3.07494930e-05, -2.08607969e-09],\n",
              "        [-4.54832083e-07,  1.59242107e-07, -2.50270314e-05, ...,\n",
              "          1.24920509e-05, -3.07494930e-05, -2.08607969e-09],\n",
              "        [-4.54832083e-07,  1.59242107e-07, -2.50270314e-05, ...,\n",
              "          1.24920509e-05, -3.07494930e-05, -2.08607969e-09]],\n",
              "       dtype=float128),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b3': array([[0.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ann(X:np.array,\n",
        "        y: np.array,\n",
        "        n_iter:int,\n",
        "        layer_dims: list,\n",
        "        learning_rate:float=0.01\n",
        "        ) -> t.Union[dict, list]:\n",
        "  \"\"\"\n",
        "  Run artificial neural network\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  X: np.array \n",
        "    A numpy array of the given data. The shape of the data should be in\n",
        "    (n_features, n_samples)\n",
        "  y: np.array\n",
        "    A numpy array of true labels\n",
        "  n_iter: int\n",
        "    Number of iterations\n",
        "  layer_dims: list\n",
        "    The number of neurons in each layer. The length of the list \n",
        "    is the number of layers including input layer\n",
        "  learning_rate: float\n",
        "    Step size of gradient descent\n",
        "  \n",
        "  Return\n",
        "  ----------\n",
        "  pred_params: dict\n",
        "    Weights of trained model \n",
        "  cost: list\n",
        "    The cost at each iteration \n",
        "  \"\"\"\n",
        "\n",
        "  params = he_initialization(layer_dims)\n",
        "  cost = []\n",
        "\n",
        "  for i in range(n_iter):\n",
        "\n",
        "    pred_params, init_params = forward_pass(X, params) \n",
        "\n",
        "    J = compute_log_loss(y, pred_params[\"a3\"])\n",
        "\n",
        "    updated_params = backpropagation(X, y, pred_params)\n",
        "\n",
        "    params = update_parameters(init_params, updated_params, learning_rate)\n",
        "\n",
        "    if i % 100 == 0: \n",
        "      print(f\"Cost after iteration {i + 100}: {J}\")\n",
        "      cost.append(J)\n",
        "\n",
        "  return pred_params, cost\n",
        "\n",
        "params, cost = ann(X_train, y_train, 15000, layer_dims=[X_train.shape[0], 4, 3, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etKN3YCZkXyI",
        "outputId": "f57b1136-9d6c-402f-eefd-a75e5321f2cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 100: 0.7044228406339578\n",
            "Cost after iteration 200: 0.6937241631840132\n",
            "Cost after iteration 300: 0.6934864286989116\n",
            "Cost after iteration 400: 0.6933782582587701\n",
            "Cost after iteration 500: 0.6933166163548018\n",
            "Cost after iteration 600: 0.693276677969123\n",
            "Cost after iteration 700: 0.6932486348965814\n",
            "Cost after iteration 800: 0.6932278327919338\n",
            "Cost after iteration 900: 0.6932118133951692\n",
            "Cost after iteration 1000: 0.6931991154101088\n",
            "Cost after iteration 1100: 0.6931887706144398\n",
            "Cost after iteration 1200: 0.6931802007849819\n",
            "Cost after iteration 1300: 0.6931729307884497\n",
            "Cost after iteration 1400: 0.6931666957904564\n",
            "Cost after iteration 1500: 0.6931613341972658\n",
            "Cost after iteration 1600: 0.6931567046593125\n",
            "Cost after iteration 1700: 0.6931526412164998\n",
            "Cost after iteration 1800: 0.6931490575407315\n",
            "Cost after iteration 1900: 0.6931458924882669\n",
            "Cost after iteration 2000: 0.6931430570763454\n",
            "Cost after iteration 2100: 0.6931405038307255\n",
            "Cost after iteration 2200: 0.6931381961276105\n",
            "Cost after iteration 2300: 0.6931360923637562\n",
            "Cost after iteration 2400: 0.6931341863558156\n",
            "Cost after iteration 2500: 0.6931324280647456\n",
            "Cost after iteration 2600: 0.6931308391529507\n",
            "Cost after iteration 2700: 0.6931294071906671\n",
            "Cost after iteration 2800: 0.6931280970341013\n",
            "Cost after iteration 2900: 0.6931268943057851\n",
            "Cost after iteration 3000: 0.6931257758791834\n",
            "Cost after iteration 3100: 0.6931247247752611\n",
            "Cost after iteration 3200: 0.6931237504348332\n",
            "Cost after iteration 3300: 0.693122854579682\n",
            "Cost after iteration 3400: 0.6931220215752313\n",
            "Cost after iteration 3500: 0.6931212472270913\n",
            "Cost after iteration 3600: 0.693120498959727\n",
            "Cost after iteration 3700: 0.6931198141458176\n",
            "Cost after iteration 3800: 0.6931191771938794\n",
            "Cost after iteration 3900: 0.6931185972747335\n",
            "Cost after iteration 4000: 0.6931180557038109\n",
            "Cost after iteration 4100: 0.6931175358575857\n",
            "Cost after iteration 4200: 0.6931170298632481\n",
            "Cost after iteration 4300: 0.6931165284148499\n",
            "Cost after iteration 4400: 0.6931160434231666\n",
            "Cost after iteration 4500: 0.6931155877090043\n",
            "Cost after iteration 4600: 0.6931151536745948\n",
            "Cost after iteration 4700: 0.6931147426522208\n",
            "Cost after iteration 4800: 0.6931143645180811\n",
            "Cost after iteration 4900: 0.6931140126931777\n",
            "Cost after iteration 5000: 0.6931136703301342\n",
            "Cost after iteration 5100: 0.6931133184728617\n",
            "Cost after iteration 5200: 0.6931129713102465\n",
            "Cost after iteration 5300: 0.6931126501426289\n",
            "Cost after iteration 5400: 0.6931123365507922\n",
            "Cost after iteration 5500: 0.6931120419675504\n",
            "Cost after iteration 5600: 0.6931117522136411\n",
            "Cost after iteration 5700: 0.6931114838144505\n",
            "Cost after iteration 5800: 0.6931112178469027\n",
            "Cost after iteration 5900: 0.6931109591192723\n",
            "Cost after iteration 6000: 0.6931107171775813\n",
            "Cost after iteration 6100: 0.6931104905381564\n",
            "Cost after iteration 6200: 0.6931102837452212\n",
            "Cost after iteration 6300: 0.693110082924722\n",
            "Cost after iteration 6400: 0.693109884201472\n",
            "Cost after iteration 6500: 0.6931096960264533\n",
            "Cost after iteration 6600: 0.6931095100695079\n",
            "Cost after iteration 6700: 0.6931093285505665\n",
            "Cost after iteration 6800: 0.6931091573239141\n",
            "Cost after iteration 6900: 0.6931089928538303\n",
            "Cost after iteration 7000: 0.6931088333597126\n",
            "Cost after iteration 7100: 0.693108690373923\n",
            "Cost after iteration 7200: 0.6931085539849389\n",
            "Cost after iteration 7300: 0.6931084198345806\n",
            "Cost after iteration 7400: 0.6931082908139276\n",
            "Cost after iteration 7500: 0.6931081649309213\n",
            "Cost after iteration 7600: 0.6931080462831216\n",
            "Cost after iteration 7700: 0.6931079282429545\n",
            "Cost after iteration 7800: 0.6931078095019392\n",
            "Cost after iteration 7900: 0.6931076914929765\n",
            "Cost after iteration 8000: 0.6931075823610505\n",
            "Cost after iteration 8100: 0.6931074763187063\n",
            "Cost after iteration 8200: 0.693107366638718\n",
            "Cost after iteration 8300: 0.693107260153119\n",
            "Cost after iteration 8400: 0.6931071572242016\n",
            "Cost after iteration 8500: 0.6931070579137223\n",
            "Cost after iteration 8600: 0.6931069592004062\n",
            "Cost after iteration 8700: 0.6931068599508661\n",
            "Cost after iteration 8800: 0.6931067618798711\n",
            "Cost after iteration 8900: 0.6931066646187839\n",
            "Cost after iteration 9000: 0.6931065694505296\n",
            "Cost after iteration 9100: 0.6931064787607013\n",
            "Cost after iteration 9200: 0.6931063882994107\n",
            "Cost after iteration 9300: 0.6931062978003181\n",
            "Cost after iteration 9400: 0.6931062068803706\n",
            "Cost after iteration 9500: 0.6931061171004\n",
            "Cost after iteration 9600: 0.6931060313739381\n",
            "Cost after iteration 9700: 0.6931059454466757\n",
            "Cost after iteration 9800: 0.6931058584344777\n",
            "Cost after iteration 9900: 0.6931057701381446\n",
            "Cost after iteration 10000: 0.6931056844701894\n",
            "Cost after iteration 10100: 0.6931056007206019\n",
            "Cost after iteration 10200: 0.6931055192906269\n",
            "Cost after iteration 10300: 0.6931054385275153\n",
            "Cost after iteration 10400: 0.6931053575110732\n",
            "Cost after iteration 10500: 0.6931052765844385\n",
            "Cost after iteration 10600: 0.693105195002636\n",
            "Cost after iteration 10700: 0.693105113066719\n",
            "Cost after iteration 10800: 0.6931050318227189\n",
            "Cost after iteration 10900: 0.6931049502601935\n",
            "Cost after iteration 11000: 0.6931048691937094\n",
            "Cost after iteration 11100: 0.6931047869157887\n",
            "Cost after iteration 11200: 0.6931047043989104\n",
            "Cost after iteration 11300: 0.6931046216605999\n",
            "Cost after iteration 11400: 0.6931045391178365\n",
            "Cost after iteration 11500: 0.6931044561209618\n",
            "Cost after iteration 11600: 0.6931043736127039\n",
            "Cost after iteration 11700: 0.6931042906834045\n",
            "Cost after iteration 11800: 0.6931042080077554\n",
            "Cost after iteration 11900: 0.6931041253214357\n",
            "Cost after iteration 12000: 0.6931040432718545\n",
            "Cost after iteration 12100: 0.6931039604017125\n",
            "Cost after iteration 12200: 0.693103877095338\n",
            "Cost after iteration 12300: 0.6931037931478512\n",
            "Cost after iteration 12400: 0.6931037086402912\n",
            "Cost after iteration 12500: 0.6931036239848881\n",
            "Cost after iteration 12600: 0.6931035398318445\n",
            "Cost after iteration 12700: 0.6931034552871114\n",
            "Cost after iteration 12800: 0.6931033712984312\n",
            "Cost after iteration 12900: 0.6931032872342603\n",
            "Cost after iteration 13000: 0.693103203128947\n",
            "Cost after iteration 13100: 0.6931031186721367\n",
            "Cost after iteration 13200: 0.6931030344298497\n",
            "Cost after iteration 13300: 0.6931029501783562\n",
            "Cost after iteration 13400: 0.6931028656464387\n",
            "Cost after iteration 13500: 0.6931027810011308\n",
            "Cost after iteration 13600: 0.6931026962207227\n",
            "Cost after iteration 13700: 0.6931026111076194\n",
            "Cost after iteration 13800: 0.6931025257382448\n",
            "Cost after iteration 13900: 0.6931024399586366\n",
            "Cost after iteration 14000: 0.6931023535970473\n",
            "Cost after iteration 14100: 0.6931022667450333\n",
            "Cost after iteration 14200: 0.6931021794481278\n",
            "Cost after iteration 14300: 0.6931020917304619\n",
            "Cost after iteration 14400: 0.6931020035184263\n",
            "Cost after iteration 14500: 0.6931019149022376\n",
            "Cost after iteration 14600: 0.6931018255803454\n",
            "Cost after iteration 14700: 0.6931017359033854\n",
            "Cost after iteration 14800: 0.6931016458004898\n",
            "Cost after iteration 14900: 0.6931015552772812\n",
            "Cost after iteration 15000: 0.6931014643904099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter = np.arange(0, 15000, 100)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "plt.plot(iter, cost)\n",
        "plt.title(\"Learning rate = 0.01\")\n",
        "plt.xlabel(\"No' iterations\")\n",
        "plt.ylabel(\"J\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "gp-Y5ula9nwi",
        "outputId": "b56c0ade-6320-4cf6-d523-0ff662ba557e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhW9qlju2QJn",
        "outputId": "36e3d05a-d283-4f01-abb4-8a71768b8912"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.69310146439040989713"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(X:np.array,\n",
        "                 y:np.array,\n",
        "                 parameters: dict\n",
        "                 ) -> float:\n",
        "    \"\"\"\n",
        "    Predict the results of a 3-layer neural network \n",
        "    Starter code: https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html\n",
        "\n",
        "    Arguments:\n",
        "    X: np.array \n",
        "      A numpy array of the given data. The shape of the data should be in\n",
        "      (n_features, n_samples)\n",
        "    y: np.array\n",
        "      A numpy array of true labels\n",
        "    parameters: dict\n",
        "      A dictionary containing weights from forward_pass()\n",
        "    \n",
        "    Returns:\n",
        "    acc: float\n",
        "      The model accuracy\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    pred = np.zeros((1, m), dtype = np.int)\n",
        "    \n",
        "    # Forward propagation\n",
        "    pred_params, _ = forward_pass(X, parameters)\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(pred_params[\"a3\"].shape[1]):\n",
        "        if pred_params[\"a3\"][0, i] > 0.5:\n",
        "            pred[0, i] = 1\n",
        "        else:\n",
        "            pred[0, i] = 0\n",
        "    \n",
        "    acc = np.mean((pred[0, :] == y[0, :]))    \n",
        "    return acc"
      ],
      "metadata": {
        "id": "dBFVEJv_r0Q5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(X_train, y_train, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4vCnu5vz52x",
        "outputId": "4abb31fd-8590-49bf-d2a6-e45f32bc2747"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.503"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}